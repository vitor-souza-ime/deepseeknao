# NAO + DeepSeek Local Conversational System

### Real-Time Speech Interaction for the NAO Humanoid Robot Using Local LLMs via Ollama

This project implements a fully local conversational architecture for the **NAO humanoid robot**, integrating the NAOqi framework with a **local Large Language Model (LLM)** provided by **Ollama** (e.g., *deepseek-llm:latest*).
The system enables real-time spoken dialogue without requiring cloud APIs, improving privacy, reducing latency, and ensuring operation even without internet access.

---

## ‚ú® Features

* **Local LLM inference** using Ollama (DeepSeek or any installed model)
* **Bidirectional communication** between NAO and the language model
* **Automatic speech recognition (ASR)** through NAO's ALSpeechRecognition
* **Natural speech synthesis (TTS)** using ALTextToSpeech
* **Expanded vocabulary support** for better speech recognition
* **Latency measurement** and performance statistics of the LLM
* **Robust dialogue loop** with failure handling and safe interruption
* **Fully autonomous offline conversation capability**

---

## üß† System Architecture

The system follows a classic pipeline for robotic conversational agents:

1. **Speech recognition (ASR)** ‚Äî NAO captures spoken input
2. **Text interpretation** ‚Äî words are aggregated into a recognized sentence
3. **LLM reasoning** ‚Äî DeepSeek or another model processes the user input
4. **Response synthesis** ‚Äî NAO speaks the model's answer
5. **Dialogue loop** ‚Äî continuous interaction with exit-word detection

All LLM processing happens **locally**, through the `ollama` CLI, avoiding cloud dependencies.

---

## üì¶ Requirements

### **Hardware**

* NAO robot (tested with NAO V6; compatible with V5)
* Local machine (Linux recommended) with:

  * 8‚Äì16 GB RAM
  * CPU with AVX/AVX2 (for faster inference)
  * Optional GPU acceleration if supported by Ollama

### **Software**

* Python ‚â• 3.8
* NAOqi Python SDK (`qi`)
* Ollama installed locally

  ```bash
  curl -fsSL https://ollama.com/install.sh | sh
  ```
* A local LLM model (example):

  ```bash
  ollama pull deepseek-llm:latest
  ```

---

## üöÄ Installation

1. Clone the repository:

   ```bash
   git clone https://github.com/your-repo/nao-deepseek-chat.git
   cd nao-deepseek-chat
   ```

2. Install Python dependencies:

   ```bash
   pip install qi
   ```

3. Verify Ollama installation:

   ```bash
   ollama --version
   ollama list
   ```

4. Pull the DeepSeek model if necessary:

   ```bash
   ollama pull deepseek-llm:latest
   ```

---

## ‚öôÔ∏è Configuration

Edit the IP address of your NAO robot in `main()`:

```python
NAO_IP = "172.15.3.253"  # replace with your robot's IP
MODEL_NAME = "deepseek-llm:latest"
```

Ensure your computer and the robot are on the same network.

---

## ‚ñ∂Ô∏è Running the System

Simply execute:

```bash
python3 nao_deepseek_chat.py
```

The robot will greet the user and start listening for speech.

To stop the conversation verbally, say any of:

```
bye, goodbye, stop, quit, exit, end
```

To stop manually, press **Ctrl+C**.

---

## üß™ Testing Speech Recognition

You can trigger a simple recognition test:

```python
chat.test_listening()
```

This verifies the microphone, ASR pipeline, and word recognition.

---

## üì° DeepSeek Integration Details

The system communicates with the model using:

```bash
ollama run model_name "prompt text"
```

It includes:

* response cleaning (removal of prefixes and tokens)
* sentence limiting (1‚Äì2 concise sentences)
* timing metrics:

  * total inference time
  * words generated
  * throughput (words/second)
  * response/input ratio

These metrics help evaluate local LLM suitability for robotics applications.

---

## üìä Performance Considerations

Local LLM inference reduces cloud latency, improving responsiveness in robotic interaction scenarios.
Typical results (may vary by hardware/model):

* **~600‚Äì900 ms** response generation for short queries
* **Robust offline operation**
* **No cloud costs**

---

## üõ° Limitations

* Local models may be less capable than large cloud-based LLMs
* Speech recognition depends on NAO‚Äôs built-in ASR limitations
* Large models require substantial hardware resources
* English only (ASR/TTS configuration), unless modified manually

---

## üìÅ Project Structure

```
üìÇ nao-deepseek-chat/
 ‚îú‚îÄ‚îÄ nao_deepseek_chat.py  # main implementation
 ‚îú‚îÄ‚îÄ README.md              # documentation
 ‚îî‚îÄ‚îÄ requirements.txt       # optional
```

---

## üìù License

MIT License ‚Äî open for academic and research use.

---

## üôå Acknowledgments

This project integrates technologies from:

* SoftBank Robotics (NAOqi API)
* Ollama (local LLM runtime)
* DeepSeek (open LLM models)


